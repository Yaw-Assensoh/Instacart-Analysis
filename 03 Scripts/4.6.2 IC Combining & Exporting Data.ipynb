{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c20e401b",
   "metadata": {},
   "source": [
    "## 4.6.2 Combining & Exporting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667aaad",
   "metadata": {},
   "source": [
    "### This script contains the following points:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efbd054",
   "metadata": {},
   "source": [
    "#### 1. Import the data sets into Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1add508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 2. Check the dimensions of the imported dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b82b395",
   "metadata": {},
   "source": [
    "#### 3. Determine a suitable way to combine the orders_products_combined dataframe with the products data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5db823",
   "metadata": {},
   "source": [
    "#### 4. Confirm the results of the merge using the merge flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969dc8eb",
   "metadata": {},
   "source": [
    "#### 5. Export the newly created dataframe as ords_prods_merge in a suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c253bca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbb62f9",
   "metadata": {},
   "source": [
    "### 1. Import the data sets into Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2276950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell Python to remember a main folder path\n",
    "path = r'/Users/giadairene/Documents/CareerFoundry Data Analytics/Data Analytics Immersion/Achievement 4/Instacart Basket Analysis'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a20c531",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/giadairene/Documents/CareerFoundry Data Analytics/Data Analytics Immersion/Achievement 4/Instacart Basket Analysis/02 Data/Prepared Data/orders_products_combined.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Import dataset orders_products_combined.pkl\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_ords_prods_combined \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m02 Data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrepared Data\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morders_products_combined.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/Users/anaconda3/lib/python3.13/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m    186\u001b[0m     filepath_or_buffer,\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    188\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m    189\u001b[0m     is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    190\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[1;32m    191\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Users/anaconda3/lib/python3.13/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/giadairene/Documents/CareerFoundry Data Analytics/Data Analytics Immersion/Achievement 4/Instacart Basket Analysis/02 Data/Prepared Data/orders_products_combined.pkl'"
     ]
    }
   ],
   "source": [
    "# Import dataset orders_products_combined.pkl\n",
    "df_ords_prods_combined = pd.read_pickle(os.path.join(path, '02 Data', 'Prepared Data', 'orders_products_combined.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441c66ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset products_checked.csv\n",
    "df_prods = pd.read_csv(os.path.join(path, '02 Data', 'Prepared Data', 'products_checked.csv'), index_col = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c973bdd4",
   "metadata": {},
   "source": [
    "### 2. Check the dimensions of the imported dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55151eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output\n",
    "df_ords_prods_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55431c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ords_prods_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3283dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output\n",
    "df_prods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26503c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df_prods = df_prods.drop(['Unnamed: 0'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f21eb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output\n",
    "df_prods.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_prods.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e36ca",
   "metadata": {},
   "source": [
    "### 3. Determine a suitable way to combine the orders_products_combined dataframe with the products data setÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8509d85",
   "metadata": {},
   "source": [
    "#### A suitable way to combine the df_ords_prods_combined dataframe with the df_prods dataframe, despite their different shape, is to merge them thanks to their shared column \"product_id\". For the purpose of this project, a default inner join is advisable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0ce52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ords_prods_merge = df_prods.merge(df_ords_prods_combined, on = 'product_id', indicator = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2842b31",
   "metadata": {},
   "source": [
    "#### I think the problem here is that the column \"merge\" already exists in the df_ords_prods_combined dataframe. Therefore, I have to drop it from the dataframe before operating the merging procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedeec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "df_ords_prods_combined = df_ords_prods_combined.drop(['_merge'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4695622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check output\n",
    "df_ords_prods_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b59805",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ords_prods_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0572c928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the updated dataframes\n",
    "df_ords_prods_merge = df_prods.merge(df_ords_prods_combined, on = 'product_id', indicator = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909f54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the output\n",
    "df_ords_prods_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e696d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ords_prods_merge.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2be3",
   "metadata": {},
   "source": [
    "### 4. Confirm the results of the merge using the merge flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d07f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ords_prods_merge['_merge'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf2f67",
   "metadata": {},
   "source": [
    "#### The resulting dataframe (after the merge) has 32,404,859 rows, and each of those rows have information found in both input data sets, as we used an inner join for the purposes of this project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fab56a",
   "metadata": {},
   "source": [
    "### 5. Export the newly created dataframe as ords_prods_merge in a suitable format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a5a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to pkl\n",
    "df_ords_prods_merge.to_pickle(os.path.join(path, '02 Data','Prepared Data', 'ords_prods_merge.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
